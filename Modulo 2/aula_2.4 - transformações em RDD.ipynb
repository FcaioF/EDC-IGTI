{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/03 00:52:36 WARN Utils: Your hostname, caio-HP-250-G8-Notebook-PC resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface wlo1)\n",
      "23/06/03 00:52:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/03 00:52:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/03 00:52:38 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n"
     ]
    }
   ],
   "source": [
    "#instanciando spark \n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('spark IGTI') \\\n",
    "    .config('spark.ui.port', '4050') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'base_dados/README.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRdd = spark.sparkContext.textFile(file_dir)\n",
    "\n",
    "#contando linhas\n",
    "linesRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('# Apache Spark', 14),\n",
       " ('', 0),\n",
       " ('Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       "  80),\n",
       " ('high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       "  75),\n",
       " ('supports general computation graphs for data analysis. It also supports a',\n",
       "  73),\n",
       " ('rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       "  74),\n",
       " ('pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,',\n",
       "  98),\n",
       " ('and Structured Streaming for stream processing.', 47),\n",
       " ('', 0),\n",
       " ('<https://spark.apache.org/>', 27),\n",
       " ('', 0),\n",
       " ('[![GitHub Actions Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)',\n",
       "  167),\n",
       " ('[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       "  189),\n",
       " ('[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
       "  123),\n",
       " ('[![PyPI Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)',\n",
       "  210),\n",
       " ('', 0),\n",
       " ('', 0),\n",
       " ('## Online Documentation', 23),\n",
       " ('', 0),\n",
       " ('You can find the latest Spark documentation, including a programming', 68),\n",
       " ('guide, on the [project web page](https://spark.apache.org/documentation.html).',\n",
       "  78),\n",
       " ('This README file only contains basic setup instructions.', 56),\n",
       " ('', 0),\n",
       " ('## Building Spark', 17),\n",
       " ('', 0),\n",
       " ('Spark is built using [Apache Maven](https://maven.apache.org/).', 63),\n",
       " ('To build Spark and its example programs, run:', 45),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./build/mvn -DskipTests clean package', 37),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('(You do not need to do this if you downloaded a pre-built package.)', 67),\n",
       " ('', 0),\n",
       " ('More detailed documentation is available from the project site, at', 66),\n",
       " ('[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       "  77),\n",
       " ('', 0),\n",
       " ('For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).',\n",
       "  157),\n",
       " ('', 0),\n",
       " ('## Interactive Scala Shell', 26),\n",
       " ('', 0),\n",
       " ('The easiest way to start using Spark is through the Scala shell:', 64),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./bin/spark-shell', 17),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('Try the following command, which should return 1,000,000,000:', 61),\n",
       " ('', 0),\n",
       " ('```scala', 8),\n",
       " ('scala> spark.range(1000 * 1000 * 1000).count()', 46),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('## Interactive Python Shell', 27),\n",
       " ('', 0),\n",
       " ('Alternatively, if you prefer Python, you can use the Python shell:', 66),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./bin/pyspark', 13),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('And run the following command, which should also return 1,000,000,000:',\n",
       "  70),\n",
       " ('', 0),\n",
       " ('```python', 9),\n",
       " ('>>> spark.range(1000 * 1000 * 1000).count()', 43),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('## Example Programs', 19),\n",
       " ('', 0),\n",
       " ('Spark also comes with several sample programs in the `examples` directory.',\n",
       "  74),\n",
       " ('To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       "  74),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./bin/run-example SparkPi', 25),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('will run the Pi example locally.', 32),\n",
       " ('', 0),\n",
       " ('You can set the MASTER environment variable when running examples to submit',\n",
       "  75),\n",
       " ('examples to a cluster. This can be a mesos:// or spark:// URL,', 62),\n",
       " ('\"yarn\" to run on YARN, and \"local\" to run', 41),\n",
       " ('locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
       "  73),\n",
       " ('can also use an abbreviated class name if the class is in the `examples`',\n",
       "  72),\n",
       " ('package. For instance:', 22),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('MASTER=spark://host:7077 ./bin/run-example SparkPi', 50),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('Many of the example programs print usage help if no params are given.', 69),\n",
       " ('', 0),\n",
       " ('## Running Tests', 16),\n",
       " ('', 0),\n",
       " ('Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       "  84),\n",
       " ('can be run using:', 17),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./dev/run-tests', 15),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('Please see the guidance on how to', 33),\n",
       " ('[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       "  110),\n",
       " ('', 0),\n",
       " ('There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md',\n",
       "  105),\n",
       " ('', 0),\n",
       " ('## A Note About Hadoop Versions', 31),\n",
       " ('', 0),\n",
       " ('Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       "  77),\n",
       " ('storage systems. Because the protocols have changed in different versions of',\n",
       "  76),\n",
       " ('Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       "  77),\n",
       " ('', 0),\n",
       " ('Please refer to the build documentation at', 42),\n",
       " ('[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       "  157),\n",
       " ('for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       "  84),\n",
       " ('building for particular Hive and Hive Thriftserver distributions.', 65),\n",
       " ('', 0),\n",
       " ('## Configuration', 16),\n",
       " ('', 0),\n",
       " ('Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
       "  98),\n",
       " ('in the online documentation for an overview on how to configure Spark.',\n",
       "  70),\n",
       " ('', 0),\n",
       " ('## Contributing', 15),\n",
       " ('', 0),\n",
       " ('Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)',\n",
       "  91),\n",
       " ('for information on how to get started contributing to the project.', 66)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map\n",
    "#esta operação quebra as linhas do texto e coloca em uma tupla juntamento com o tamanho da linha em questão\n",
    "mapRdd = linesRdd.map(lambda line: (line, len(line)))\n",
    "mapRdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'unified',\n",
       " 'analytics',\n",
       " 'engine',\n",
       " 'for',\n",
       " 'large-scale',\n",
       " 'data',\n",
       " 'processing.',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'high-level',\n",
       " 'APIs',\n",
       " 'in',\n",
       " 'Scala,',\n",
       " 'Java,',\n",
       " 'Python,',\n",
       " 'and',\n",
       " 'R,',\n",
       " 'and',\n",
       " 'an',\n",
       " 'optimized',\n",
       " 'engine',\n",
       " 'that',\n",
       " 'supports',\n",
       " 'general',\n",
       " 'computation',\n",
       " 'graphs',\n",
       " 'for',\n",
       " 'data',\n",
       " 'analysis.',\n",
       " 'It',\n",
       " 'also',\n",
       " 'supports',\n",
       " 'a',\n",
       " 'rich',\n",
       " 'set',\n",
       " 'of',\n",
       " 'higher-level',\n",
       " 'tools',\n",
       " 'including',\n",
       " 'Spark',\n",
       " 'SQL',\n",
       " 'for',\n",
       " 'SQL',\n",
       " 'and',\n",
       " 'DataFrames,',\n",
       " 'pandas',\n",
       " 'API',\n",
       " 'on',\n",
       " 'Spark',\n",
       " 'for',\n",
       " 'pandas',\n",
       " 'workloads,',\n",
       " 'MLlib',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning,',\n",
       " 'GraphX',\n",
       " 'for',\n",
       " 'graph',\n",
       " 'processing,',\n",
       " 'and',\n",
       " 'Structured',\n",
       " 'Streaming',\n",
       " 'for',\n",
       " 'stream',\n",
       " 'processing.',\n",
       " '<https://spark.apache.org/>',\n",
       " '[![GitHub',\n",
       " 'Actions',\n",
       " 'Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)',\n",
       " '[![AppVeyor',\n",
       " 'Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       " '[![PySpark',\n",
       " 'Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
       " '[![PyPI',\n",
       " 'Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)',\n",
       " '##',\n",
       " 'Online',\n",
       " 'Documentation',\n",
       " 'You',\n",
       " 'can',\n",
       " 'find',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'Spark',\n",
       " 'documentation,',\n",
       " 'including',\n",
       " 'a',\n",
       " 'programming',\n",
       " 'guide,',\n",
       " 'on',\n",
       " 'the',\n",
       " '[project',\n",
       " 'web',\n",
       " 'page](https://spark.apache.org/documentation.html).',\n",
       " 'This',\n",
       " 'README',\n",
       " 'file',\n",
       " 'only',\n",
       " 'contains',\n",
       " 'basic',\n",
       " 'setup',\n",
       " 'instructions.',\n",
       " '##',\n",
       " 'Building',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built',\n",
       " 'using',\n",
       " '[Apache',\n",
       " 'Maven](https://maven.apache.org/).',\n",
       " 'To',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'and',\n",
       " 'its',\n",
       " 'example',\n",
       " 'programs,',\n",
       " 'run:',\n",
       " '```bash',\n",
       " './build/mvn',\n",
       " '-DskipTests',\n",
       " 'clean',\n",
       " 'package',\n",
       " '```',\n",
       " '(You',\n",
       " 'do',\n",
       " 'not',\n",
       " 'need',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " 'if',\n",
       " 'you',\n",
       " 'downloaded',\n",
       " 'a',\n",
       " 'pre-built',\n",
       " 'package.)',\n",
       " 'More',\n",
       " 'detailed',\n",
       " 'documentation',\n",
       " 'is',\n",
       " 'available',\n",
       " 'from',\n",
       " 'the',\n",
       " 'project',\n",
       " 'site,',\n",
       " 'at',\n",
       " '[\"Building',\n",
       " 'Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For',\n",
       " 'general',\n",
       " 'development',\n",
       " 'tips,',\n",
       " 'including',\n",
       " 'info',\n",
       " 'on',\n",
       " 'developing',\n",
       " 'Spark',\n",
       " 'using',\n",
       " 'an',\n",
       " 'IDE,',\n",
       " 'see',\n",
       " '[\"Useful',\n",
       " 'Developer',\n",
       " 'Tools\"](https://spark.apache.org/developer-tools.html).',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Scala',\n",
       " 'Shell',\n",
       " 'The',\n",
       " 'easiest',\n",
       " 'way',\n",
       " 'to',\n",
       " 'start',\n",
       " 'using',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'through',\n",
       " 'the',\n",
       " 'Scala',\n",
       " 'shell:',\n",
       " '```bash',\n",
       " './bin/spark-shell',\n",
       " '```',\n",
       " 'Try',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'return',\n",
       " '1,000,000,000:',\n",
       " '```scala',\n",
       " 'scala>',\n",
       " 'spark.range(1000',\n",
       " '*',\n",
       " '1000',\n",
       " '*',\n",
       " '1000).count()',\n",
       " '```',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Python',\n",
       " 'Shell',\n",
       " 'Alternatively,',\n",
       " 'if',\n",
       " 'you',\n",
       " 'prefer',\n",
       " 'Python,',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'the',\n",
       " 'Python',\n",
       " 'shell:',\n",
       " '```bash',\n",
       " './bin/pyspark',\n",
       " '```',\n",
       " 'And',\n",
       " 'run',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'also',\n",
       " 'return',\n",
       " '1,000,000,000:',\n",
       " '```python',\n",
       " '>>>',\n",
       " 'spark.range(1000',\n",
       " '*',\n",
       " '1000',\n",
       " '*',\n",
       " '1000).count()',\n",
       " '```',\n",
       " '##',\n",
       " 'Example',\n",
       " 'Programs',\n",
       " 'Spark',\n",
       " 'also',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'several',\n",
       " 'sample',\n",
       " 'programs',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'directory.',\n",
       " 'To',\n",
       " 'run',\n",
       " 'one',\n",
       " 'of',\n",
       " 'them,',\n",
       " 'use',\n",
       " '`./bin/run-example',\n",
       " '<class>',\n",
       " '[params]`.',\n",
       " 'For',\n",
       " 'example:',\n",
       " '```bash',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " '```',\n",
       " 'will',\n",
       " 'run',\n",
       " 'the',\n",
       " 'Pi',\n",
       " 'example',\n",
       " 'locally.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'set',\n",
       " 'the',\n",
       " 'MASTER',\n",
       " 'environment',\n",
       " 'variable',\n",
       " 'when',\n",
       " 'running',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'a',\n",
       " 'cluster.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'mesos://',\n",
       " 'or',\n",
       " 'spark://',\n",
       " 'URL,',\n",
       " '\"yarn\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'on',\n",
       " 'YARN,',\n",
       " 'and',\n",
       " '\"local\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'one',\n",
       " 'thread,',\n",
       " 'or',\n",
       " '\"local[N]\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'N',\n",
       " 'threads.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'use',\n",
       " 'an',\n",
       " 'abbreviated',\n",
       " 'class',\n",
       " 'name',\n",
       " 'if',\n",
       " 'the',\n",
       " 'class',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'package.',\n",
       " 'For',\n",
       " 'instance:',\n",
       " '```bash',\n",
       " 'MASTER=spark://host:7077',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " '```',\n",
       " 'Many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'example',\n",
       " 'programs',\n",
       " 'print',\n",
       " 'usage',\n",
       " 'help',\n",
       " 'if',\n",
       " 'no',\n",
       " 'params',\n",
       " 'are',\n",
       " 'given.',\n",
       " '##',\n",
       " 'Running',\n",
       " 'Tests',\n",
       " 'Testing',\n",
       " 'first',\n",
       " 'requires',\n",
       " '[building',\n",
       " 'Spark](#building-spark).',\n",
       " 'Once',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built,',\n",
       " 'tests',\n",
       " 'can',\n",
       " 'be',\n",
       " 'run',\n",
       " 'using:',\n",
       " '```bash',\n",
       " './dev/run-tests',\n",
       " '```',\n",
       " 'Please',\n",
       " 'see',\n",
       " 'the',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " '[run',\n",
       " 'tests',\n",
       " 'for',\n",
       " 'a',\n",
       " 'module,',\n",
       " 'or',\n",
       " 'individual',\n",
       " 'tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       " 'There',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'Kubernetes',\n",
       " 'integration',\n",
       " 'test,',\n",
       " 'see',\n",
       " 'resource-managers/kubernetes/integration-tests/README.md',\n",
       " '##',\n",
       " 'A',\n",
       " 'Note',\n",
       " 'About',\n",
       " 'Hadoop',\n",
       " 'Versions',\n",
       " 'Spark',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'core',\n",
       " 'library',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'HDFS',\n",
       " 'and',\n",
       " 'other',\n",
       " 'Hadoop-supported',\n",
       " 'storage',\n",
       " 'systems.',\n",
       " 'Because',\n",
       " 'the',\n",
       " 'protocols',\n",
       " 'have',\n",
       " 'changed',\n",
       " 'in',\n",
       " 'different',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'you',\n",
       " 'must',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'against',\n",
       " 'the',\n",
       " 'same',\n",
       " 'version',\n",
       " 'that',\n",
       " 'your',\n",
       " 'cluster',\n",
       " 'runs.',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'build',\n",
       " 'documentation',\n",
       " 'at',\n",
       " '[\"Specifying',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'Version',\n",
       " 'and',\n",
       " 'Enabling',\n",
       " 'YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       " 'for',\n",
       " 'detailed',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'building',\n",
       " 'for',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'including',\n",
       " 'building',\n",
       " 'for',\n",
       " 'particular',\n",
       " 'Hive',\n",
       " 'and',\n",
       " 'Hive',\n",
       " 'Thriftserver',\n",
       " 'distributions.',\n",
       " '##',\n",
       " 'Configuration',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " '[Configuration',\n",
       " 'Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in',\n",
       " 'the',\n",
       " 'online',\n",
       " 'documentation',\n",
       " 'for',\n",
       " 'an',\n",
       " 'overview',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'configure',\n",
       " 'Spark.',\n",
       " '##',\n",
       " 'Contributing',\n",
       " 'Please',\n",
       " 'review',\n",
       " 'the',\n",
       " '[Contribution',\n",
       " 'to',\n",
       " 'Spark',\n",
       " 'guide](https://spark.apache.org/contributing.html)',\n",
       " 'for',\n",
       " 'information',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'contributing',\n",
       " 'to',\n",
       " 'the',\n",
       " 'project.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "flatmap \n",
    "\n",
    "quebrando as linhas em palavras\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "FlatmapRdd = linesRdd.flatMap(lambda line: (line.split()))\n",
    "FlatmapRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'analytics',\n",
       " 'and',\n",
       " 'and',\n",
       " 'an',\n",
       " 'analysis.',\n",
       " 'also',\n",
       " 'a',\n",
       " 'and',\n",
       " 'and',\n",
       " 'a',\n",
       " 'and',\n",
       " 'a',\n",
       " 'available',\n",
       " 'at',\n",
       " 'an',\n",
       " 'also',\n",
       " 'also',\n",
       " 'a',\n",
       " 'a',\n",
       " 'and',\n",
       " 'also',\n",
       " 'an',\n",
       " 'abbreviated',\n",
       " 'are',\n",
       " 'a',\n",
       " 'also',\n",
       " 'a',\n",
       " 'and',\n",
       " 'against',\n",
       " 'at',\n",
       " 'and',\n",
       " 'a',\n",
       " 'and',\n",
       " 'an']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Filtrando apenas palavras que comece com a letra A\n",
    "\"\"\"\n",
    "\n",
    "filterRdd = (\n",
    "    linesRdd\n",
    "    .flatMap(lambda line: line.split())\n",
    "    .filter(lambda word: word.startswith('a'))\n",
    ")\n",
    "\n",
    "filterRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('um', 2), ('dois', 1), ('tres', 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contando quantas vezes as palavras aparecem dentro do dataset\n",
    "\"\"\"\n",
    "count_words = ['um','dois','tres','um','tres']\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(count_words)\n",
    "\n",
    "rdd2 = (\n",
    "    rdd\n",
    "    .map(lambda word: (word,1))\n",
    "    .reduceByKey(lambda a,b: a+b)\n",
    ")\n",
    "\n",
    "rdd2.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dois', 1), ('tres', 2), ('um', 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contando quantas vezes as palavras aparecem dentro do dataset e ordenando pela chave\n",
    "\"\"\"\n",
    "count_words = ['um','dois','tres','um','tres']\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(count_words)\n",
    "\n",
    "rdd2 = (\n",
    "    rdd\n",
    "    .map(lambda word: (word,1))\n",
    "    .reduceByKey(lambda a,b: a+b)\n",
    "    .sortByKey('asc')\n",
    ")\n",
    "\n",
    "rdd2.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dois', 1), ('um', 2), ('tres', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contando quantas vezes as palavras aparecem dentro do dataset e ordenando pelo segundo valor da chave\n",
    "\"\"\"\n",
    "count_words = ['um','dois','tres','um','tres']\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(count_words)\n",
    "\n",
    "rdd2 = (\n",
    "    rdd\n",
    "    .map(lambda word: (word,1))\n",
    "    .reduceByKey(lambda a,b: a+b)\n",
    "    .sortBy(lambda t: t[1])\n",
    ")\n",
    "\n",
    "rdd2.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um', 'um', 'dois', 'tres', 'quatro', 'cinco']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fazendo a união de dois datasets RDD em um unico\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df_1 = ['um','um','dois','tres']\n",
    "df_2 = ['quatro','cinco']\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(df_1)\n",
    "rdd2 = spark.sparkContext.parallelize(df_2)\n",
    "\n",
    "rddUnion = rdd1.union(rdd2)\n",
    "\n",
    "rddUnion.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um', 'quatro']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "localizando o item que interocorre entre os dois dadaset (localiza quais valores aparecem em ambos os datasets)\n",
    "\"\"\"\n",
    "\n",
    "df_1 = ['um','um','dois','tres','quatro']\n",
    "\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(df_1)\n",
    "rdd2 = spark.sparkContext.parallelize(df_2)\n",
    "\n",
    "rddUnion = rdd1.intersection(rdd2)\n",
    "\n",
    "rddUnion.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um', 'dois', 'tres', 'quatro']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "localizando os item distintos no dataset\n",
    "\"\"\"\n",
    "\n",
    "df_1 = ['um','um','dois','tres','quatro']\n",
    "\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(df_1)\n",
    "\n",
    "\n",
    "rddDistinct = rdd1.distinct()\n",
    "\n",
    "rddDistinct.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Istefania', (19, 'SP')), ('caio', (21, 'SP'))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Operação de junção entre datasets (JOIN)\n",
    "\"\"\"\n",
    "\n",
    "lista1 = [('caio',21),('Istefania',19)]\n",
    "lista2 = [('caio','SP'),('Istefania','SP'),('Joao','CE')]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(lista1)\n",
    "rdd2 = spark.sparkContext.parallelize(lista2)\n",
    "\n",
    "rddJoin = rdd1.join(rdd2)\n",
    "\n",
    "rddJoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#contando valores\n",
    "\n",
    "rddJoin.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Istefania', (19, 'SP'))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resgatando valores aleatorios do rdd TAKE\n",
    "\n",
    "rddJoin.take(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um', 'um', 'tres']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resgantando os 3 primeiro valores (TOP)\n",
    "\"\"\"\n",
    "\n",
    "df_1 = ['um','um','dois','tres']\n",
    "df_2 = ['quatro','cinco']\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(df_1)\n",
    "rdd2 = spark.sparkContext.parallelize(df_2)\n",
    "\n",
    "rddUnion = rdd1.union(rdd2)\n",
    "\n",
    "rddUnion.collect()\n",
    "\n",
    "rddUnion.top(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'um': 2, 'dois': 1, 'tres': 1, 'quatro': 1, 'cinco': 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contando quantas vezes cada registro ocorreu\n",
    "\"\"\"\n",
    "\n",
    "df_1 = ['um','um','dois','tres']\n",
    "df_2 = ['quatro','cinco']\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(df_1)\n",
    "rdd2 = spark.sparkContext.parallelize(df_2)\n",
    "\n",
    "rddUnion = rdd1.union(rdd2)\n",
    "\n",
    "rddUnion.collect()\n",
    "\n",
    "rddUnion.countByValue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gravando as transformações do rdd como um arquivo\n",
    "\"\"\"\n",
    "rddUnion.saveAsTextFile('base_dados/saida2.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
